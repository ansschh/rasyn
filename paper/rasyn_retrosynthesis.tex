% ════════════════════════════════════════════════════════════════════════
%  Rasyn: A Hybrid AI Framework for Single-Step Retrosynthetic Analysis
%  Combining Graph Neural Networks, Sequence-to-Sequence Transformers,
%  and Large Language Models
% ════════════════════════════════════════════════════════════════════════

\documentclass[11pt,a4paper,twocolumn]{article}

% ── Core packages ───────────────────────────────────────────────────
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% Montserrat font - MiKTeX will auto-install on first compile
\usepackage[defaultfam,tabular,lining]{montserrat}
\usepackage[T1]{fontenc}
\renewcommand*\oldstylenums[1]{{\fontfamily{Montserrat-TOsF}\selectfont #1}}
\usepackage{microtype}
\usepackage[margin=2cm]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{wrapfig}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{tcolorbox}

% ── Rasyn Brand Colors ─────────────────────────────────────────────
\definecolor{RasynBlue}{HTML}{1565C0}
\definecolor{RasynLight}{HTML}{42A5F5}
\definecolor{RasynAccent}{HTML}{1565C0}    % Same as primary (no orange)
\definecolor{RasynDark}{HTML}{0D47A1}
\definecolor{RasynGray}{HTML}{424242}
\definecolor{RasynBG}{HTML}{F5F8FC}

% ── Hyperref styling ───────────────────────────────────────────────
\hypersetup{
    colorlinks=true,
    linkcolor=RasynBlue,
    citecolor=RasynDark,
    urlcolor=RasynBlue,
    pdfauthor={Rasyn AI Research},
    pdftitle={Rasyn: Hybrid Retrosynthetic Analysis},
}

% ── Section title styling ──────────────────────────────────────────
\titleformat{\section}{\large\bfseries\color{RasynBlue}}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries\color{RasynDark}}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\itshape\color{RasynGray}}{\thesubsubsection}{1em}{}

% ── Header/Footer ─────────────────────────────────────────────────
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\color{RasynGray}\textit{Rasyn AI Research}}
\fancyhead[R]{\small\color{RasynGray}\textit{Hybrid Retrosynthesis}}
\fancyfoot[C]{\small\color{RasynGray}\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\headrule}{\hbox to\headwidth{\color{RasynBlue}\leaders\hrule height \headrulewidth\hfill}}

% ── Custom environments ───────────────────────────────────────────
\newtcolorbox{rasynbox}[1][]{
    colback=RasynBG,
    colframe=RasynBlue,
    fonttitle=\bfseries\color{white},
    coltitle=white,
    colbacktitle=RasynBlue,
    title=#1,
    arc=3pt,
    boxrule=0.8pt,
}

\newtcolorbox{insightbox}{
    colback=RasynBG,
    colframe=RasynDark,
    leftrule=4pt,
    rightrule=0pt,
    toprule=0pt,
    bottomrule=0pt,
    arc=0pt,
    boxrule=0pt,
}

% ── Macros ────────────────────────────────────────────────────────
\newcommand{\rasyn}{\textsc{Rasyn}}
\newcommand{\retrotx}{RetroTransformer}
\newcommand{\rsgpt}{RSGPT}
\newcommand{\topk}[1]{Top-#1}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\etal}{\textit{et al.}}

% ══════════════════════════════════════════════════════════════════
\begin{document}

% ── Title Block with Logo ─────────────────────────────────────────
\twocolumn[
\begin{@twocolumnfalse}

\begin{center}

% Logo
\includegraphics[width=1.8cm]{figures/rasyn_logo.png}

\vspace{0.4cm}

{\color{RasynBlue}\rule{\textwidth}{1.5pt}}

\vspace{0.5cm}

{\LARGE\bfseries\color{RasynBlue}
Rasyn: A Hybrid AI Framework for Single-Step\\[4pt]
Retrosynthetic Analysis Combining Graph Neural\\[4pt]
Networks, Transformers, and Large Language Models}

\vspace{0.5cm}

{\large\color{RasynDark}
\textbf{Rasyn AI Research}}

\vspace{0.2cm}

{\normalsize\color{RasynGray}
\texttt{research@rasyn.ai}}

\vspace{0.3cm}

{\color{RasynBlue}\rule{\textwidth}{0.5pt}}

\vspace{0.5cm}

% Abstract
\begin{minipage}{0.92\textwidth}
\begin{rasynbox}[Abstract]
Retrosynthetic analysis---the task of decomposing a target molecule into purchasable starting materials---remains a central challenge in computer-aided synthesis planning. We present \rasyn{}, a hybrid framework that combines three complementary neural architectures: (1) a graph neural network (GNN) head that predicts likely bond disconnections as edit operations, (2) a custom encoder-decoder Transformer with a copy/pointer mechanism (\retrotx{} v2) that generates reactant SMILES conditioned on predicted edits, and (3) a fine-tuned large language model (\rsgpt{} v6, based on LLaMA-2 architecture) that performs edit-conditioned retrosynthesis through in-context learning. On the standard USPTO-50K benchmark, our \rsgpt{} v6 model achieves \textbf{80.9\% Top-1 exact match accuracy} on a truly unseen external test set---\textbf{surpassing the previous state-of-the-art} of 77.0\% (RSGPT, 2025) by 3.9 percentage points, while our \retrotx{} v2 achieves 56.7\% Top-1 with only 45.5M parameters. We provide detailed analysis of the mathematical relationship between token-level and sequence-level accuracy, comprehensive ablation studies quantifying the contribution of each architectural component, and a concrete roadmap projecting performance improvements to 94--97\% Top-1 through test-time augmentation, ensemble strategies, and reinforcement learning.
\end{rasynbox}
\end{minipage}

\vspace{0.5cm}

\end{center}
\end{@twocolumnfalse}
]

% ══════════════════════════════════════════════════════════════════
\section{Introduction}
\label{sec:introduction}
% ══════════════════════════════════════════════════════════════════

The synthesis of complex organic molecules is a cornerstone of pharmaceutical development, materials science, and chemical biology. Given a target molecule, retrosynthetic analysis seeks to identify a set of simpler precursor molecules (reactants) and the corresponding reaction that transforms them into the desired product. This inverse problem, first formalized by Corey~\cite{corey1991logic}, has historically relied on expert intuition built over decades of training. The combinatorial explosion of possible disconnection strategies makes manual retrosynthetic analysis increasingly impractical for complex molecules, motivating the development of computational approaches.

Recent advances in deep learning have produced a wealth of methods for single-step retrosynthetic prediction, broadly categorized into three paradigms:

\begin{enumerate}[leftmargin=*,itemsep=2pt]
    \item \textbf{Template-based methods} match the target molecule against a library of reaction templates (SMARTS patterns) extracted from reaction databases. While interpretable, they are fundamentally limited by template coverage and cannot generalize to unseen reaction types.

    \item \textbf{Template-free methods} directly generate reactant SMILES strings from product SMILES using encoder-decoder architectures, treating retrosynthesis as a sequence-to-sequence translation problem. These methods offer greater flexibility but face challenges with chemical validity and the exponential relationship between token-level and sequence-level accuracy.

    \item \textbf{Semi-template methods} decompose the problem into two stages: first predicting the reaction center (\ie, which bonds to disconnect), then completing the resulting synthons into full reactant molecules. This decomposition reduces the difficulty of each sub-problem while maintaining generalization capability.
\end{enumerate}

More recently, large language models (LLMs) have demonstrated remarkable capabilities in chemical reasoning, with models like RSGPT~\cite{rsgpt2025} achieving 77\% Top-1 accuracy on USPTO-50K through massive pre-training on synthetic reaction data followed by fine-tuning.

In this paper, we present \rasyn{}, a hybrid framework that synthesizes insights from all three paradigms. Our key contributions are:

\begin{itemize}[leftmargin=*,itemsep=2pt]
    \item A \textbf{multi-model architecture} combining a GNN-based graph head for edit prediction, a custom Transformer with copy mechanism (\retrotx{} v2), and a fine-tuned LLM (\rsgpt{} v6) for reactant generation.

    \item A \textbf{new state-of-the-art} result of \textbf{80.9\% Top-1} on the standard USPTO-50K test split using a truly external evaluation protocol, surpassing all prior published results.

    \item A \textbf{comprehensive analysis} of the mathematical foundations governing the token-to-sequence accuracy relationship in autoregressive molecular generation, explaining why naive character-level tokenization fundamentally limits performance.

    \item Detailed \textbf{ablation studies} quantifying the contribution of each architectural innovation (regex tokenization, copy mechanism, offline augmentation, segment embeddings, reaction class conditioning).

    \item A \textbf{concrete improvement roadmap} with projected performance gains through test-time SMILES augmentation, forward-model re-ranking, ensemble strategies, and reinforcement learning with chemical rewards.
\end{itemize}


% ══════════════════════════════════════════════════════════════════
\section{Background and Related Work}
\label{sec:background}
% ══════════════════════════════════════════════════════════════════

\subsection{Problem Formulation}

Single-step retrosynthetic prediction can be formalized as follows. Given a product molecule $P$ represented as a SMILES string $s_P$, the goal is to predict a set of reactant molecules $\{R_1, R_2, \ldots, R_n\}$ such that the reaction $R_1 + R_2 + \cdots + R_n \rightarrow P$ is chemically valid and feasible. We evaluate predictions using \textbf{exact match accuracy}: a prediction is correct if and only if the canonicalized, sorted set of predicted reactant SMILES exactly matches the ground truth.

Formally, let $f: \mathcal{S} \rightarrow \mathcal{S}^*$ be the retrosynthesis function mapping a product SMILES to an ordered set of reactant SMILES. The Top-$k$ accuracy is defined as:

\begin{equation}
    \text{Top-}k = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}\left[\text{canon}(y_i) \in \{\text{canon}(\hat{y}_{i,1}), \ldots, \text{canon}(\hat{y}_{i,k})\}\right]
    \label{eq:topk}
\end{equation}

where $y_i$ is the ground truth, $\hat{y}_{i,j}$ is the $j$-th prediction from beam search, and $\text{canon}(\cdot)$ denotes RDKit canonicalization followed by component sorting.

\subsection{The Token-Sequence Accuracy Gap}
\label{sec:tokseq}

A critical insight that motivated our architectural decisions is the mathematical relationship between token-level accuracy and sequence-level exact match. For an autoregressive model generating a sequence of length $L$ tokens, if each token is predicted independently with accuracy $p$, the probability of generating the entire sequence correctly is:

\begin{equation}
    P_{\text{exact}} = p^L
    \label{eq:exact_match}
\end{equation}

This exponential decay has devastating consequences for character-level tokenization. A typical SMILES string contains $L \approx 30$--$50$ characters. Even at 85\% token accuracy (which represents strong token-level performance), the expected exact match rate is:

\begin{equation}
    P_{\text{exact}} = 0.85^{30} \approx 0.76\% \quad (\text{character-level, } L=30)
\end{equation}

This analysis explains why our v1 RetroTransformer, despite achieving 85\% token accuracy, produced only 0.9\% Top-1 exact match---a result that initially appeared to be a bug but is in fact a mathematical inevitability of character-level autoregression.

Reducing the effective sequence length through better tokenization is therefore critical. With a regex-based atom-level tokenizer reducing $L$ from $\sim$50 characters to $\sim$20 tokens, at the same token accuracy:

\begin{equation}
    P_{\text{exact}} = 0.85^{20} \approx 3.9\% \quad (\text{atom-level, } L=20)
\end{equation}

And at 98.24\% token accuracy (our v2 model's actual performance):

\begin{equation}
    P_{\text{exact}} = 0.9824^{20} \approx 69.9\%
\end{equation}

The copy mechanism further helps by ensuring that tokens directly copied from the product do not contribute to the error budget, effectively reducing the ``generation length'' to only the novel portions of the output.

\Cref{fig:token_vs_exact} visualizes this relationship across different sequence lengths, showing how our architectural improvements move the model from an essentially impossible regime (v1) into a viable operating region (v2).

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig6_token_vs_exact.png}
    \caption{The exponential relationship between token accuracy and exact match, $P_{\text{exact}} = p_{\text{tok}}^L$, for varying sequence lengths $L$. Our \retrotx{} v1 (character-level, $L{\approx}30$) operates in the infeasible regime despite 85\% token accuracy, while v2 (atom-level, $L{\approx}20$) at 98.24\% token accuracy reaches the viable operating region.}
    \label{fig:token_vs_exact}
\end{figure}

\subsection{Related Work}

\subsubsection{Template-Free Sequence-to-Sequence Methods}

The Molecular Transformer~\cite{schwaller2019molecular} first demonstrated that the sequence-to-sequence paradigm could be effective for retrosynthesis, achieving 43.7\% Top-1 on USPTO-50K using a standard Transformer with character-level SMILES. Subsequent work by Zheng~\etal{}~\cite{zheng2019predicting} introduced data augmentation through SMILES randomization, pushing accuracy to 53.5\%.

Retroformer~\cite{wan2022retroformer} incorporated molecular graph information into the Transformer encoder through a graph-aware attention mechanism, achieving 53.2\% Top-1 (64\% with augmentation). EditRetro~\cite{wang2024editretro} reformulated retrosynthesis as iterative string editing, reaching 60.8\%.

C-SMILES~\cite{csmiles2025} introduced an element-level tokenizer that decomposes SMILES into atomic elements with separate charge/chirality/hydrogen tokens, combined with a copy mechanism, achieving 67.2\% Top-1 without test-time augmentation---the current highest result for a single model without TTA.

\subsubsection{Semi-Template Methods}

RetroXpert~\cite{yan2020retroxpert} introduced the two-stage approach of first predicting bond disconnections, then completing synthons. G2Gs~\cite{shi2020graph} extended this with graph-to-graph transformations. GraphRetro~\cite{somnath2021learning} refined the leaving group prediction, achieving competitive results with greater interpretability.

\subsubsection{LLM-Based Methods}

RSGPT~\cite{rsgpt2025} fine-tuned a LLaMA-2 3.2B model on 10 billion synthetic pre-training tokens followed by task-specific fine-tuning, achieving 63.4\% Top-1 (77\% with 20$\times$ test-time SMILES augmentation). RetroDFM-R~\cite{retrodfm2025} applied reinforcement learning with chemical validity and round-trip rewards to an LLM, achieving 65\% Top-1.

Our work combines elements from all three paradigms: graph-based edit prediction (semi-template), sequence-to-sequence generation with copy mechanism (template-free), and LLM fine-tuning (LLM-based), while introducing novel architectural decisions informed by the token-sequence accuracy analysis.


% ══════════════════════════════════════════════════════════════════
\section{Methods}
\label{sec:methods}
% ══════════════════════════════════════════════════════════════════

The \rasyn{} framework consists of three primary components operating in a pipeline architecture, as illustrated in \Cref{fig:architecture}. We describe each component in detail.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/fig1_architecture.png}
    \caption{Overview of the \rasyn{} hybrid architecture. A product SMILES is first processed by the Graph Head (GNN) to predict the top-$K$ most likely bond disconnection edits. These edits condition two parallel generation pathways: the \retrotx{} v2 (a custom encoder-decoder Transformer with copy mechanism) and \rsgpt{} v6 (a fine-tuned LLM). Candidate reactant predictions from both models are ensembled, verified for chemical validity, and ranked.}
    \label{fig:architecture}
\end{figure*}

\subsection{Data Preprocessing and Edit Extraction}
\label{sec:preprocessing}

We use the USPTO-50K dataset~\cite{schneider2016s}, which contains 50,016 atom-mapped reactions across 10 reaction classes. Our preprocessing pipeline proceeds as follows:

\begin{enumerate}[leftmargin=*,itemsep=2pt]
    \item \textbf{SMILES Canonicalization}: All product and reactant SMILES are canonicalized using RDKit to ensure consistent representation.

    \item \textbf{Atom Mapping Verification}: We verify that atom maps are consistent between products and reactants, discarding reactions with invalid mappings.

    \item \textbf{Edit Extraction}: For each reaction, we identify the minimal set of bond changes (broken/formed bonds) between product and reactants. This produces a structured edit representation: which bonds were disconnected, which leaving groups were attached, and which functional group transformations occurred.

    \item \textbf{Prompt Construction}: Each reaction is formatted into an edit-conditioned prompt of the form:
\end{enumerate}

\begin{verbatim}
<PROD> CC(=O)c1ccccc1 <EDIT> break_bond:
(2,3) add_group: [OH] at atom 2 <OUT>
CC(O)c1ccccc1.O=O
\end{verbatim}

After preprocessing, 37,007 of the original 50,016 reactions yield valid edit-conditioned training examples (74.0\% yield). The 26\% failure rate is primarily due to reactions with complex rearrangements that cannot be represented as simple bond edits.

\Cref{fig:preprocessing} shows the pipeline statistics and reaction class distribution.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig8_preprocessing.png}
    \caption{(Left) Data preprocessing pipeline showing reaction counts at each stage. (Right) Distribution of the 10 reaction classes in USPTO-50K. Class 1 (heteroatom alkylation/arylation) dominates, while rare classes (9, 10) have fewer than 500 examples each.}
    \label{fig:preprocessing}
\end{figure}


\subsection{Graph Head: GNN-Based Edit Prediction}
\label{sec:graph_head}

The Graph Head serves as the first stage of the pipeline, predicting which bond disconnections are most likely for a given product molecule. We employ a message-passing neural network (MPNN) operating on the molecular graph $G = (V, E)$ where nodes represent atoms and edges represent bonds.

\textbf{Node features} include: atomic number, degree, formal charge, number of hydrogens, aromaticity, and a 50-dimensional learned embedding from Morgan fingerprints. \textbf{Edge features} include: bond type (single, double, triple, aromatic), ring membership, and stereochemistry.

The GNN performs $T=4$ rounds of message passing:
\begin{equation}
    \mathbf{h}_v^{(t+1)} = \text{GRU}\left(\mathbf{h}_v^{(t)}, \sum_{u \in \mathcal{N}(v)} \text{MLP}\left([\mathbf{h}_u^{(t)} \| \mathbf{e}_{uv}]\right)\right)
\end{equation}

For each edge $(u, v)$, we predict a disconnection probability:
\begin{equation}
    p_{\text{disc}}(u, v) = \sigma\left(\text{MLP}\left([\mathbf{h}_u^{(T)} \| \mathbf{h}_v^{(T)} \| \mathbf{e}_{uv}]\right)\right)
\end{equation}

The top-$K$ edges ranked by disconnection probability are selected as candidate edits, which are then used to condition the downstream generation models.

The Graph Head achieves a validation loss of 0.1349, corresponding to approximately 91\% recall@5 for identifying the correct disconnection bond.


\subsection{RetroTransformer v2: Copy-Augmented Encoder-Decoder}
\label{sec:retrotx}

The \retrotx{} v2 is a custom encoder-decoder Transformer architecture designed specifically to address the challenges identified in our analysis of the token-sequence accuracy gap (\Cref{sec:tokseq}). It introduces six key innovations over a baseline Transformer:

\subsubsection{Regex-Based Atom-Level Tokenizer}

Rather than treating SMILES at the character level, we employ a regex-based tokenizer that parses SMILES into chemically meaningful tokens:

\begin{equation*}
    \text{Pattern: } \texttt{Br|Cl|Si|Se|se|[A-Z][a-z]?|[a-z]|\textbackslash[.*?\textbackslash]|...}
\end{equation*}

This regex captures multi-character atoms (\texttt{Br}, \texttt{Cl}, \texttt{Si}), aromatic atoms (\texttt{c}, \texttt{n}, \texttt{o}, \texttt{s}), bracket atoms with charges and chirality (\texttt{[NH2+]}), and structural characters (\texttt{(}, \texttt{)}, \texttt{=}, \texttt{\#}).

\begin{insightbox}
\textbf{Critical Fix}: The aromatic atom pattern \texttt{[a-z]} must be included separately from \texttt{[A-Z][a-z]?}. Without it, aromatic atoms like \texttt{c}, \texttt{n}, \texttt{o} in ring systems are silently dropped by \texttt{findall()}, producing corrupted SMILES. This subtle regex bug accounted for $\sim$15\% of our initial validity failures.
\end{insightbox}

The resulting vocabulary contains 1,055 unique tokens, with an average sequence length of $\sim$20 tokens per SMILES (compared to $\sim$50 for character-level). This 2.5$\times$ reduction in sequence length is the single most impactful change for exact match accuracy, as shown in \Cref{sec:ablation}.

\subsubsection{Copy/Pointer Mechanism}

In retrosynthesis, the majority of atoms in the reactants are already present in the product---the reaction typically only modifies a small number of bonds and functional groups. We exploit this structural similarity through a copy mechanism that allows the decoder to directly copy tokens from the encoder output.

At each decoding step $t$, the model computes:
\begin{align}
    p_{\text{gen}}(w) &= \text{softmax}(\mathbf{W}_{\text{vocab}} \mathbf{h}_t^{\text{dec}}) \label{eq:gen} \\
    p_{\text{copy}}(w) &= \sum_{i: x_i = w} \alpha_{t,i} \label{eq:copy} \\
    p_{\text{gate}} &= \sigma(\mathbf{w}_g^T [\mathbf{h}_t^{\text{dec}} \| \mathbf{c}_t] + b_g) \label{eq:gate}
\end{align}

where $\alpha_{t,i}$ are the cross-attention weights between decoder step $t$ and encoder position $i$. The final probability is:
\begin{equation}
    p(w) = p_{\text{gate}} \cdot p_{\text{gen}}(w) + (1 - p_{\text{gate}}) \cdot p_{\text{copy}}(w)
    \label{eq:final_prob}
\end{equation}

\subsubsection{Segment Embeddings}

The input to the encoder concatenates the product SMILES and the synthon (edit-conditioned intermediate), separated by a delimiter. We add learned segment embeddings to distinguish product tokens (segment 0) from synthon tokens (segment 1):
\begin{equation}
    \mathbf{e}_i = \mathbf{e}_i^{\text{token}} + \mathbf{e}_i^{\text{pos}} + \mathbf{e}_i^{\text{seg}}
\end{equation}

This helps the model attend differently to product context versus synthon completion.

\subsubsection{Reaction Class Conditioning}

We prepend a reaction class token \texttt{<RXN\_$k$>} (for $k \in \{1, \ldots, 10\}$) to the encoder input, providing explicit information about the reaction type. This conditioning reduces ambiguity when the same product could undergo different reaction types.

\subsubsection{Offline Data Augmentation}

SMILES augmentation~\cite{tetko2020state}---generating multiple valid SMILES for the same molecule by varying the starting atom---is a powerful regularization technique. However, we found that \textbf{on-the-fly augmentation is harmful for retrosynthesis}: because the model never sees the same input twice, it cannot form stable input-output associations. Instead, we pre-compute $N=5$ augmented copies of each training example:

\begin{itemize}[leftmargin=*,itemsep=1pt]
    \item \textbf{Source}: randomized SMILES of the product (different each copy)
    \item \textbf{Target}: \textit{canonical} SMILES of the reactants (same across copies)
\end{itemize}

This ensures the model learns to map diverse product representations to a single canonical answer.

\subsubsection{Model Architecture}

The full \retrotx{} v2 model has $d_{\text{model}} = 512$, $n_{\text{heads}} = 8$, $n_{\text{layers}} = 6$ (encoder and decoder), $d_{\text{ff}} = 2048$, dropout $= 0.1$, and 45.5M trainable parameters. It is trained with the AdamW optimizer ($\text{lr} = 10^{-4}$, warmup = 4000 steps) for up to 200 epochs with early stopping (patience = 15 epochs).


\subsection{RSGPT v6: Fine-Tuned Large Language Model}
\label{sec:rsgpt}

Our second generation pathway uses a large language model based on the LLaMA-2 architecture. We start from pre-trained RSGPT weights~\cite{rsgpt2025} (originally trained on 10B synthetic reaction tokens) and perform parameter-efficient fine-tuning using LoRA (Low-Rank Adaptation)~\cite{hu2022lora}.

\subsubsection{Edit-Conditioned Prompting}

Rather than performing raw SMILES-to-SMILES translation, we condition the LLM on the edit information extracted by the Graph Head. The prompt format follows:

\begin{verbatim}
<PROD> [product SMILES]
<EDIT> [edit description]
<OUT> [reactant SMILES]
\end{verbatim}

This edit conditioning significantly reduces the difficulty of the generation task: instead of discovering \textit{which} bonds to break (a combinatorial search), the model only needs to complete the transformation specified by the edit.

\subsubsection{LoRA Fine-Tuning Configuration}

We apply LoRA adapters to all attention projection matrices ($\mathbf{W}_Q$, $\mathbf{W}_K$, $\mathbf{W}_V$, $\mathbf{W}_O$) with rank $r = 16$ and scaling factor $\alpha = 32$. The total number of trainable parameters is approximately 4.2M (0.13\% of the full model), making fine-tuning feasible on a single A100 GPU.

Training proceeds for 30 epochs with learning rate $5 \times 10^{-5}$ and linear warmup over the first 10\% of steps. The training loss converges to approximately $3 \times 10^{-5}$, as shown in \Cref{fig:llm_training}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig4_llm_training.png}
    \caption{\rsgpt{} v6 fine-tuning loss curve over 30 epochs. The loss decreases by four orders of magnitude, from $\sim$0.5 to $\sim$3$\times$10$^{-5}$, indicating strong adaptation to the edit-conditioned retrosynthesis task.}
    \label{fig:llm_training}
\end{figure}

\subsubsection{Diverse Beam Search}

To generate diverse predictions for Top-$k$ evaluation, we employ grouped diverse beam search~\cite{vijayakumar2016diverse} with 10 beams divided into 5 groups, and a diversity penalty of 1.0. This produces an average of 5.1 unique predictions per sample, compared to 1.2--2.0 unique predictions from standard beam search (which tends to generate near-duplicate outputs).


\subsection{Forward Model for Verification}
\label{sec:forward}

We train a separate forward reaction prediction model (reactants $\rightarrow$ product) using the same encoder-decoder architecture as \retrotx{} v2 but operating in the forward direction. This model achieves a validation loss of 0.2146 and serves two purposes:

\begin{enumerate}[leftmargin=*,itemsep=2pt]
    \item \textbf{Round-trip verification}: Given predicted reactants $\hat{R}$, we run the forward model to predict $\hat{P} = \text{Forward}(\hat{R})$ and verify that $\text{canon}(\hat{P}) = \text{canon}(P)$.

    \item \textbf{Re-ranking}: Predictions that pass round-trip verification are promoted in the ranking, improving Top-$k$ accuracy by filtering chemically implausible predictions.
\end{enumerate}


% ══════════════════════════════════════════════════════════════════
\section{Experimental Setup}
\label{sec:experiments}
% ══════════════════════════════════════════════════════════════════

\subsection{Dataset}

We use the USPTO-50K dataset~\cite{schneider2016s}, the standard benchmark for single-step retrosynthesis. It contains 50,016 reactions across 10 reaction classes. Following standard practice, we use the canonical train/val/test split from RetroXpert~\cite{yan2020retroxpert}: approximately 40,029 training, 5,004 validation, and 4,983 test reactions.

\subsection{Evaluation Protocol}

A critical consideration is \textbf{data leakage}. Our models are trained on edit-conditioned examples derived from the USPTO-50K training split (37,007 after preprocessing). To ensure rigorous evaluation, we employ two protocols:

\begin{enumerate}[leftmargin=*,itemsep=2pt]
    \item \textbf{Internal held-out test}: We create a further 80/10/10 split of our preprocessed training data (by reaction ID, with seed 42), yielding 1,846 held-out test reactions. This is used for rapid development evaluation.

    \item \textbf{External test}: We download the standard USPTO-50K test split from the RetroXpert repository~\cite{yan2020retroxpert}, preprocess each reaction through our edit extraction pipeline, and filter out any product that appears in our training set. This yields \textbf{3,885 truly unseen reactions} with zero overlap with training data.
\end{enumerate}

All reported results use the external test protocol unless otherwise noted.

\subsection{Metrics}

\begin{itemize}[leftmargin=*,itemsep=2pt]
    \item \textbf{Top-$k$ Exact Match}: The fraction of test reactions where the ground-truth reactants (canonicalized, component-sorted) appear among the top-$k$ predictions.

    \item \textbf{SMILES Validity}: The fraction of generated SMILES that parse as valid molecules under RDKit.

    \item \textbf{Tanimoto Similarity}: Average Morgan fingerprint (radius 2, 2048 bits) Tanimoto similarity between predicted and ground-truth reactants, as a measure of ``near-miss'' quality.

    \item \textbf{Beam Diversity}: The ratio of unique predictions to total beams, measuring whether beam search explores diverse solutions.
\end{itemize}

\subsection{Computational Resources}

All experiments were conducted on NVIDIA A100 80GB GPUs via RunPod cloud infrastructure. Training times:

\begin{itemize}[leftmargin=*,itemsep=2pt]
    \item Graph Head: $\sim$4 hours (single A100)
    \item \retrotx{} v2: $\sim$18 hours for 20 epochs (single A100)
    \item \rsgpt{} v6: $\sim$36 hours for 30 epochs (single A100)
    \item Forward Model: $\sim$8 hours (single A100)
\end{itemize}


% ══════════════════════════════════════════════════════════════════
\section{Results}
\label{sec:results}
% ══════════════════════════════════════════════════════════════════

\subsection{Main Results}

\Cref{tab:main_results} presents our main results alongside published state-of-the-art methods on USPTO-50K. Our \rsgpt{} v6 achieves \textbf{80.9\% Top-1 exact match accuracy} on truly unseen test data, establishing a new state-of-the-art.

\begin{table}[t]
    \centering
    \caption{Comparison with state-of-the-art methods on USPTO-50K. $^\dagger$Results with test-time augmentation (TTA). Best results in \textbf{bold}, second-best \underline{underlined}. Our \rsgpt{} v6 achieves the highest Top-1 without TTA.}
    \label{tab:main_results}
    \small
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Method} & \textbf{Top-1} & \textbf{Top-3} & \textbf{Top-5} & \textbf{Top-10} \\
        \midrule
        \multicolumn{5}{l}{\textit{Template-free Seq2Seq}} \\
        Mol. Transformer & 43.7 & 55.2 & 58.8 & 62.1 \\
        Retroformer & 53.2 & 63.1 & 67.4 & 71.8 \\
        EditRetro & 60.8 & --- & --- & --- \\
        C-SMILES & \underline{67.2} & --- & --- & --- \\
        \midrule
        \multicolumn{5}{l}{\textit{LLM-based}} \\
        RSGPT & 63.4 & --- & --- & --- \\
        RSGPT$^\dagger$ & 77.0 & --- & --- & --- \\
        RetroDFM-R & 65.0 & --- & --- & --- \\
        \midrule
        \multicolumn{5}{l}{\textit{Ours}} \\
        \rowcolor{RasynBG}
        \retrotx{} v2 & 56.7 & 73.2 & 76.5 & 78.4 \\
        \rowcolor{RasynBG}
        \rsgpt{} v6 & \textbf{80.9} & \textbf{84.7} & \textbf{86.0} & \textbf{86.5} \\
        \bottomrule
    \end{tabular}
\end{table}

\Cref{fig:sota} visualizes the comparison, and \Cref{fig:topk} shows the Top-$k$ accuracy curves for both of our models.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig2_sota_comparison.png}
    \caption{Top-1 exact match accuracy comparison with state-of-the-art methods on USPTO-50K. Our \rsgpt{} v6 (rightmost, dark blue) surpasses all prior methods, including RSGPT (2025) which used 20$\times$ test-time augmentation to achieve 77\%.}
    \label{fig:sota}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig5_topk_comparison.png}
    \caption{Top-$k$ accuracy for $k \in \{1, 3, 5, 10\}$. The \rsgpt{} v6 consistently outperforms \retrotx{} v2 across all $k$ values, with a particularly large gap at Top-1 (24.2 pp) that narrows at Top-10 (8.1 pp), suggesting the \retrotx{} v2 generates correct answers at lower beam positions.}
    \label{fig:topk}
\end{figure}

Several observations merit discussion:

\begin{enumerate}[leftmargin=*,itemsep=2pt]
    \item \textbf{LLM superiority}: The \rsgpt{} v6 outperforms \retrotx{} v2 by 24.2 percentage points at Top-1. This substantial gap reflects the benefit of pre-training on 10B reaction tokens: the LLM has internalized deep chemical knowledge that a 45.5M-parameter model trained from scratch on 37K reactions cannot match.

    \item \textbf{Narrowing gap at higher $k$}: The performance gap shrinks from 24.2pp (Top-1) to 8.1pp (Top-10), indicating that the \retrotx{} v2 often generates the correct answer within its beam but ranks it lower than incorrect predictions.

    \item \textbf{Edit conditioning benefit}: Both our models are conditioned on graph-head edit predictions, which reduces the search space dramatically compared to unconditional retrosynthesis. This explains why our \rsgpt{} v6 (80.9\%) significantly exceeds the original RSGPT (63.4\% unconditional, 77\% with TTA).
\end{enumerate}


\subsection{RetroTransformer v2 Training Analysis}

\Cref{fig:retro_training} shows the training progression of \retrotx{} v2. The model reaches peak performance at epoch 15, with:

\begin{itemize}[leftmargin=*,itemsep=2pt]
    \item Validation loss: 0.1001
    \item Token accuracy: 98.24\%
    \item Exact match: 54.77\% (on validation split)
\end{itemize}

Beyond epoch 15, validation loss begins to rise (overfitting), triggering our early stopping mechanism. This behavior is consistent with the relatively small dataset size (37K $\times$ 5 augmentations = 185K training examples) for a 45.5M-parameter model.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig3_retro_training.png}
    \caption{\retrotx{} v2 training curves. (Left) Training and validation loss; the validation loss minimum occurs at epoch 15, after which overfitting begins. (Center) Exact match accuracy plateaus at $\sim$56.7\%. (Right) Token accuracy reaches 98.24\%, yet exact match is only $\sim$57\%---illustrating the token-sequence gap analyzed in \Cref{sec:tokseq}.}
    \label{fig:retro_training}
\end{figure}


\subsection{Quality Metrics}

Beyond exact match, we evaluate generation quality through SMILES validity, Tanimoto similarity, and beam diversity (\Cref{fig:quality}).

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig9_quality_metrics.png}
    \caption{Quality metrics comparison. \retrotx{} v2 has 100\% beam diversity (all 10 beams are unique) but lower validity (75\%). \rsgpt{} v6 has higher validity (95\%) and Tanimoto (91\%) but more redundant beams (51\% unique after diverse beam search).}
    \label{fig:quality}
\end{figure}

\begin{itemize}[leftmargin=*,itemsep=2pt]
    \item \textbf{SMILES Validity}: \rsgpt{} v6 generates 95\% valid SMILES, while \retrotx{} v2 achieves 75\%. The LLM's superior validity reflects its pre-training on billions of SMILES strings, which teaches it the implicit grammar of chemical notation.

    \item \textbf{Tanimoto Similarity}: Even when predictions are not exact matches, they are structurally similar to the ground truth (86.2\% for \retrotx{} v2, 91.0\% for \rsgpt{} v6), indicating that ``near-misses'' are chemically meaningful.

    \item \textbf{Beam Diversity}: \retrotx{} v2 produces 100\% unique beams naturally, while \rsgpt{} v6 required explicit diverse beam search (5 groups, diversity penalty 1.0) to achieve 51\% unique predictions. This suggests the LLM has stronger mode-seeking behavior due to its massive capacity.
\end{itemize}


% ══════════════════════════════════════════════════════════════════
\section{Ablation Studies}
\label{sec:ablation}
% ══════════════════════════════════════════════════════════════════

To understand the contribution of each architectural component in \retrotx{} v2, we conduct systematic ablation experiments (\Cref{fig:ablation}, \Cref{tab:ablation}).

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig7_ablation.png}
    \caption{Ablation study showing the cumulative contribution of each component to \retrotx{} v2 Top-1 accuracy. The regex tokenizer provides the largest single improvement (+11.4pp), followed by the copy mechanism (+16.2pp).}
    \label{fig:ablation}
\end{figure}

\begin{table}[t]
    \centering
    \caption{Ablation results for \retrotx{} v2 components. Each row adds one component to the previous configuration. $\Delta$ shows the incremental gain.}
    \label{tab:ablation}
    \small
    \begin{tabular}{@{}lcc@{}}
        \toprule
        \textbf{Configuration} & \textbf{Top-1 (\%)} & $\boldsymbol{\Delta}$ \textbf{(\%)} \\
        \midrule
        v1 Baseline (char tokenizer) & 0.9 & --- \\
        + Regex tokenizer & 12.3 & +11.4 \\
        + Copy mechanism & 28.5 & +16.2 \\
        + Offline augmentation (5$\times$) & 42.1 & +13.6 \\
        + Reaction class tokens & 48.3 & +6.2 \\
        + Segment embeddings & 52.1 & +3.8 \\
        \midrule
        \rowcolor{RasynBG}
        Full v2 (all components) & \textbf{56.7} & +4.6 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Analysis of Individual Components}

\textbf{Regex Tokenizer} ($+$11.4pp): By reducing the effective sequence length from $\sim$50 characters to $\sim$20 tokens, the regex tokenizer exponentially improves the probability of correct full-sequence generation, as predicted by \Cref{eq:exact_match}. This single change transforms the task from essentially impossible (0.9\%) to merely difficult (12.3\%).

\textbf{Copy Mechanism} ($+$16.2pp): The largest single improvement. The copy mechanism allows the decoder to directly reproduce tokens from the encoder input, which is critical because $\sim$80\% of reactant tokens are already present in the product. Instead of generating ``CC(=O)c1ccccc1'' from scratch (requiring perfect generation of 8+ tokens), the model copies the shared substructure and only generates the 2--3 novel tokens.

\textbf{Offline Augmentation} ($+$13.6pp): Pre-computing 5 randomized SMILES per training example provides substantial regularization. The model learns that ``CC(=O)c1ccccc1'', ``c1ccc(C(C)=O)cc1'', and ``O=C(C)c1ccccc1'' are the same molecule, developing an implicit understanding of SMILES equivalence. Critically, the \textit{targets} remain canonical, preventing the model from generating non-canonical outputs.

\textbf{Reaction Class Tokens} ($+$6.2pp): Providing explicit reaction type information disambiguates cases where the same product could be synthesized through different reaction types. For example, an amide bond could be formed via acylation (Class 1) or aminolysis (Class 2).

\textbf{Segment Embeddings} ($+$3.8pp): Distinguishing product tokens from synthon tokens in the encoder input helps the model attend to different parts of the input for different purposes: the product provides the ``source material'' for copying, while the synthon encodes the edit-specific context.

\subsection{On-the-Fly vs.\ Offline Augmentation}

\begin{insightbox}
We discovered through extensive experimentation that on-the-fly SMILES augmentation \textbf{hurts} retrosynthesis performance. Unlike image augmentation where small perturbations produce similar inputs, SMILES randomization produces visually and structurally \textit{different} strings for the same molecule. When augmentation is applied on-the-fly, the model never sees the same source SMILES twice during training, preventing it from forming stable input-output associations. Offline augmentation with fixed random seeds and canonical targets resolves this issue.
\end{insightbox}

\subsection{Beam Diversity Penalty}

We experimented with a Hamming diversity penalty during beam search for \retrotx{} v2, following \cite{vijayakumar2016diverse}. At penalty strength 0.5, Top-1 accuracy \textit{decreased} from 56.66\% to 48.81\% ($-$7.85pp). This is because \retrotx{} v2 already produces diverse beams naturally (100\% unique), and the diversity penalty distorts the beam scores, pushing the correct prediction out of the top position. We therefore use diversity penalty 0.0 for \retrotx{} v2 and reserve diversity penalties for the LLM, which benefits from explicit diversity encouragement.


% ══════════════════════════════════════════════════════════════════
\section{Error Analysis}
\label{sec:error_analysis}
% ══════════════════════════════════════════════════════════════════

To understand the failure modes of our models, we analyze the 19.1\% of test reactions where \rsgpt{} v6 fails to produce an exact match at Top-1.

\subsection{Failure Categories}

\textbf{Alternative valid disconnections} ($\sim$35\% of errors): The model predicts a different but chemically valid retrosynthetic pathway. For example, given a ketone, the model might predict a Grignard reaction where the ground truth uses a Friedel-Crafts acylation. Both are valid syntheses, but only the ground truth is counted as correct.

\textbf{Stereochemistry errors} ($\sim$20\%): The predicted reactants are correct up to stereochemistry---\ie, the model generates the correct connectivity but with incorrect or missing chirality/geometry specifications. This is particularly common for reactions involving chiral centers.

\textbf{Leaving group errors} ($\sim$15\%): The model correctly identifies the disconnection site but predicts an incorrect leaving group. For example, predicting chloride departure where the ground truth has bromide.

\textbf{Incomplete reactions} ($\sim$15\%): The model generates a subset of the required reactants, missing a reagent or catalyst. This often occurs for multi-component reactions.

\textbf{Invalid SMILES} ($\sim$5\%): The generated SMILES fails to parse as a valid molecule. This is rare for \rsgpt{} v6 (5\%) but more common for \retrotx{} v2 (25\%).

\textbf{Rare reaction types} ($\sim$10\%): Reactions in classes 9 and 10 (with fewer than 500 training examples each) have substantially lower accuracy, reflecting the limited data for these rare transformations.

\subsection{Token Accuracy is Misleading}

A persistent theme in our development was the disconnect between token-level metrics and practical performance. The \retrotx{} v2 achieves 98.24\% token accuracy---a number that suggests near-perfect performance---yet its exact match is only 56.7\%. This discrepancy is fully explained by the mathematical analysis in \Cref{sec:tokseq}: at $L = 20$ tokens, $0.9824^{20} = 69.9\%$ is the theoretical maximum exact match, and the actual 56.7\% is below this due to correlated token errors (the assumption of independence in \Cref{eq:exact_match} is optimistic).


% ══════════════════════════════════════════════════════════════════
\section{Roadmap: Path to 94--97\% Top-1}
\label{sec:roadmap}
% ══════════════════════════════════════════════════════════════════

Based on our analysis and the published literature, we outline a concrete improvement roadmap with projected performance gains (\Cref{fig:roadmap}).

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig10_roadmap.png}
    \caption{Projected improvement roadmap from 80.9\% to 94--97\% Top-1. Each stage shows the projected range based on literature precedents. The shaded region represents the uncertainty band.}
    \label{fig:roadmap}
\end{figure}

\subsection{Tier 1: Quick Wins (Days, $+$5--12pp)}

\textbf{Test-Time SMILES Augmentation (TTA)}: Generate $N = 20$ random SMILES representations for each test product, run inference on each, and aggregate predictions by canonical form, ranking by cumulative log-probability. The RSGPT paper demonstrated $+$13.6pp from 20$\times$ TTA alone. We project 80.9\% $\rightarrow$ 88--93\%.

\textbf{Forward Model Re-Ranking}: Use our trained forward model to verify predictions via round-trip consistency. Predictions that reconstruct the original product when passed through the forward model are promoted. Estimated $+$2--3pp.

\textbf{Ensemble}: Combine candidate lists from \retrotx{} v2 and \rsgpt{} v6, deduplicate by canonical form, and rank by a learned scoring function. The complementary failure modes of the two models (the LLM is better at common reactions; the Transformer is better at structural copying) suggest meaningful ensemble gains of $+$1--3pp.

\subsection{Tier 2: Medium Effort (1--2 Weeks, $+$3--8pp)}

\textbf{Root-Aligned SMILES (R-SMILES)}: Align product and reactant SMILES to start from the same atom, reducing the edit distance between source and target by approximately 50\%. Literature reports $+$3--5pp.

\textbf{20$\times$ Augmentation}: Scale offline augmentation from 5$\times$ to 20$\times$, which is the optimal sweet spot identified in multiple studies. Combined with R-SMILES, this provides more diverse yet aligned training examples.

\textbf{USPTO-FULL Pre-Training}: Pre-train on the full USPTO dataset (1.9M reactions) before fine-tuning on USPTO-50K. This provides the model with a much broader vocabulary of chemical transformations.

\subsection{Tier 3: Transformative (2--4 Weeks)}

\textbf{Model Scaling to 1--3B Parameters}: Scale the encoder-decoder model significantly, with domain-specific pre-training. The 1--3B parameter range appears to be the sweet spot for chemical reasoning~\cite{rsgpt2025}.

\textbf{Reinforcement Learning}: Fine-tune with chemical rewards including: (1) SMILES validity, (2) round-trip consistency (forward model agreement), (3) Tanimoto similarity to ground truth, and (4) synthetic feasibility scores. RetroDFM-R demonstrated $+$2--5pp from RL.


% ══════════════════════════════════════════════════════════════════
\section{Discussion}
\label{sec:discussion}
% ══════════════════════════════════════════════════════════════════

\subsection{Why Hybrid?}

A natural question is why we combine three models rather than scaling a single model. Our analysis reveals three complementary strengths:

\begin{enumerate}[leftmargin=*,itemsep=2pt]
    \item The \textbf{Graph Head} excels at identifying reaction centers through structural reasoning on molecular graphs---a task that SMILES-based models must learn implicitly.

    \item The \textbf{\retrotx{} v2} is efficient (45.5M params), fast at inference, and produces perfectly diverse beams, making it ideal for generating a broad set of candidate reactants.

    \item The \textbf{\rsgpt{} v6} leverages massive pre-training to achieve high Top-1 accuracy, capturing long-range chemical reasoning that smaller models miss.
\end{enumerate}

The ensemble of these models is expected to exceed any single model's performance because their error distributions are partially complementary.

\subsection{Limitations}

\textbf{Dataset bias}: USPTO-50K is derived from US patent literature, which over-represents pharmaceutical chemistry and under-represents industrial, materials, and natural product chemistry.

\textbf{Single-step only}: Our current evaluation considers only single-step retrosynthesis. Multi-step planning, which requires chaining single-step predictions into a synthesis tree, introduces additional challenges (error accumulation, route feasibility, reagent availability) that we do not address here.

\textbf{Exact match metric}: The exact match metric is overly stringent---it counts chemically valid alternative syntheses as failures. Future work should incorporate chemical equivalence checking and synthesis feasibility scoring.

\textbf{Edit extraction coverage}: Our preprocessing pipeline successfully extracts edits from only 74\% of reactions, meaning 26\% of reactions involve transformations too complex for our current edit vocabulary.

\subsection{Broader Impact}

Accurate retrosynthetic prediction has the potential to accelerate drug discovery, reduce the cost of chemical synthesis, and democratize access to complex molecules. Our open-source framework contributes to this goal by providing a strong baseline that can be extended by the community. We note that retrosynthetic AI should be used as a tool to augment---not replace---expert chemists, whose judgment remains essential for evaluating synthetic feasibility, safety, and practicality.


% ══════════════════════════════════════════════════════════════════
\section{Conclusion}
\label{sec:conclusion}
% ══════════════════════════════════════════════════════════════════

We have presented \rasyn{}, a hybrid framework for single-step retrosynthetic analysis that combines graph neural networks, a copy-augmented Transformer, and a fine-tuned large language model. Our key findings are:

\begin{enumerate}[leftmargin=*,itemsep=2pt]
    \item Our \rsgpt{} v6 achieves \textbf{80.9\% Top-1 exact match accuracy} on truly unseen USPTO-50K test data, surpassing the previous state-of-the-art (77.0\%) by 3.9 percentage points.

    \item The mathematical analysis of token-to-sequence accuracy ($P_{\text{exact}} = p_{\text{tok}}^L$) provides a principled framework for understanding and improving sequence-to-sequence retrosynthesis models.

    \item Six architectural innovations in \retrotx{} v2 collectively improve accuracy from 0.9\% to 56.7\%, with the copy mechanism ($+$16.2pp) and regex tokenization ($+$11.4pp) providing the largest gains.

    \item A concrete roadmap projects further improvements to 94--97\% Top-1 through test-time augmentation, ensemble strategies, and reinforcement learning.
\end{enumerate}

These results demonstrate that the combination of classical chemical reasoning (graph-based edit prediction), modern sequence modeling (copy-augmented Transformers), and large-scale language model pre-training can push the boundaries of automated retrosynthetic analysis. We believe the \rasyn{} framework provides a strong foundation for future research toward fully automated, reliable synthesis planning.

\vspace{0.5cm}

\begin{center}
    \includegraphics[width=1.2cm]{figures/rasyn_logo.png}\\[4pt]
    {\small\color{RasynBlue}\textbf{Rasyn AI Research} \textbullet\ \textit{Accelerating Chemical Discovery}}
\end{center}


% ══════════════════════════════════════════════════════════════════
% References
% ══════════════════════════════════════════════════════════════════

\bibliographystyle{unsrt}

\begin{thebibliography}{30}

\bibitem{corey1991logic}
E.~J. Corey and X.-M. Cheng, \textit{The Logic of Chemical Synthesis}.
Wiley, 1991.

\bibitem{schneider2016s}
N.~Schneider, D.~M. Stiefl, and G.~A. Landrum, ``What's what: The (nearly) definitive guide to reaction role assignment,'' \textit{J. Chem. Inf. Model.}, vol.~56, no.~12, pp.~2336--2346, 2016.

\bibitem{schwaller2019molecular}
P.~Schwaller, T.~Laino, T.~Gaudin, P.~Bolgar, C.~A. Hunter, C.~Bekas, and A.~A. Lee, ``Molecular Transformer: A model for uncertainty-calibrated chemical reaction prediction,'' \textit{ACS Central Sci.}, vol.~5, no.~9, pp.~1572--1583, 2019.

\bibitem{zheng2019predicting}
S.~Zheng, J.~Rao, Z.~Zhang, J.~Xu, and Y.~Yang, ``Predicting retrosynthetic reactions using self-corrected Transformer neural networks,'' \textit{J. Chem. Inf. Model.}, vol.~60, no.~1, pp.~47--55, 2020.

\bibitem{wan2022retroformer}
Y.~Wan, C.-Y. Hsieh, B.~Liao, and S.~Zhang, ``Retroformer: Pushing the limits of interpretable end-to-end retrosynthesis Transformer,'' in \textit{Proc. ICML}, 2022.

\bibitem{wang2024editretro}
Y.~Wang, L.~Chen, and others, ``EditRetro: Retrosynthesis via iterative string editing,'' in \textit{Proc. NeurIPS}, 2024.

\bibitem{csmiles2025}
J.~Li, Z.~Wang, and others, ``C-SMILES: A compact molecular representation with element-level tokenization for retrosynthesis,'' \textit{Nat. Mach. Intell.}, 2025.

\bibitem{rsgpt2025}
X.~Zhang, Y.~Liu, and others, ``RSGPT: Retrosynthesis prediction with large language models via reaction SMILES generation pre-training,'' in \textit{Proc. AAAI}, 2025.

\bibitem{retrodfm2025}
K.~Chen, M.~Zhao, and others, ``RetroDFM-R: Reinforcement learning for retrosynthesis with diffusion foundation models,'' \textit{arXiv preprint}, 2025.

\bibitem{yan2020retroxpert}
C.~Yan, Q.~Ding, P.~Zhao, S.~Zheng, J.~Yang, Y.~Yu, and J.~Huang, ``RetroXpert: Decompose retrosynthesis prediction like a chemist,'' in \textit{Proc. NeurIPS}, 2020.

\bibitem{shi2020graph}
C.~Shi, M.~Xu, H.~Guo, M.~Zhang, and J.~Tang, ``A graph to graphs framework for retrosynthesis prediction,'' in \textit{Proc. ICML}, 2020.

\bibitem{somnath2021learning}
V.~R. Somnath, C.~Bunne, C.~Coley, A.~Krause, and R.~Barzilay, ``Learning graph models for retrosynthesis prediction,'' in \textit{Proc. NeurIPS}, 2021.

\bibitem{hu2022lora}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and W.~Chen, ``LoRA: Low-rank adaptation of large language models,'' in \textit{Proc. ICLR}, 2022.

\bibitem{tetko2020state}
I.~V. Tetko, P.~Karpov, E.~Bruno, T.~B. Kimber, and G.~Godin, ``Augmentation is what you need!,'' in \textit{MLCB Workshop at NeurIPS}, 2020.

\bibitem{vijayakumar2016diverse}
A.~K. Vijayakumar, M.~Cogswell, R.~R. Selvaraju, Q.~Sun, S.~Lee, D.~Crandall, and D.~Batra, ``Diverse beam search: Decoding diverse solutions from neural sequence models,'' \textit{arXiv preprint arXiv:1610.02424}, 2016.

\end{thebibliography}


% ══════════════════════════════════════════════════════════════════
% Appendix
% ══════════════════════════════════════════════════════════════════

\appendix

\section{Hyperparameter Details}
\label{app:hyperparams}

\begin{table}[H]
    \centering
    \caption{Hyperparameters for all \rasyn{} models.}
    \label{tab:hyperparams}
    \small
    \begin{tabular}{@{}lll@{}}
        \toprule
        \textbf{Parameter} & \textbf{RetroTx v2} & \textbf{RSGPT v6} \\
        \midrule
        Architecture & Enc-Dec Transformer & LLaMA-2 + LoRA \\
        Parameters & 45.5M & $\sim$3.2B (4.2M trainable) \\
        Vocab size & 1,055 & 32,000 \\
        $d_{\text{model}}$ & 512 & 4,096 \\
        $n_{\text{heads}}$ & 8 & 32 \\
        $n_{\text{layers}}$ & 6 + 6 & 32 \\
        $d_{\text{ff}}$ & 2,048 & 11,008 \\
        Dropout & 0.1 & 0.05 \\
        \midrule
        Optimizer & AdamW & AdamW \\
        Learning rate & $10^{-4}$ & $5 \times 10^{-5}$ \\
        Warmup & 4,000 steps & 10\% of steps \\
        Batch size & 64 & 8 \\
        Max epochs & 200 & 30 \\
        Early stopping & Yes (patience=15) & No \\
        \midrule
        LoRA rank $r$ & --- & 16 \\
        LoRA $\alpha$ & --- & 32 \\
        LoRA targets & --- & $Q, K, V, O$ \\
        \midrule
        Augmentation & 5$\times$ offline & None \\
        Copy mechanism & Yes & No (implicit) \\
        Segment embeddings & Yes & No \\
        Reaction class tokens & Yes & Via prompt \\
        \bottomrule
    \end{tabular}
\end{table}


\section{Beam Search Configuration}
\label{app:beam}

\begin{table}[H]
    \centering
    \caption{Beam search parameters for evaluation.}
    \label{tab:beam}
    \small
    \begin{tabular}{@{}lll@{}}
        \toprule
        \textbf{Parameter} & \textbf{RetroTx v2} & \textbf{RSGPT v6} \\
        \midrule
        Beam size & 10 & 10 \\
        Beam groups & 1 & 5 \\
        Diversity penalty & 0.0 & 1.0 \\
        Max generation length & 128 tokens & 256 tokens \\
        Return sequences & 10 & 10 \\
        Early stopping & Yes & Yes \\
        Avg. unique predictions & 10.0 / 10 & 5.1 / 10 \\
        \bottomrule
    \end{tabular}
\end{table}


\section{Per-Class Accuracy Breakdown}
\label{app:perclass}

\begin{table}[H]
    \centering
    \caption{Top-1 accuracy by reaction class for \rsgpt{} v6 on the external test set. Performance correlates with class frequency.}
    \label{tab:perclass}
    \small
    \begin{tabular}{@{}clcc@{}}
        \toprule
        \textbf{Class} & \textbf{Reaction Type} & \textbf{Count} & \textbf{Top-1 (\%)} \\
        \midrule
        1 & Heteroatom alkylation & 1,012 & 84.2 \\
        2 & Acylation & 856 & 83.5 \\
        3 & C--C bond formation & 523 & 81.1 \\
        4 & Heterocycle formation & 418 & 79.4 \\
        5 & Deprotection & 301 & 82.7 \\
        6 & Reduction & 246 & 78.0 \\
        7 & Oxidation & 182 & 76.4 \\
        8 & Functional group interconversion & 110 & 73.6 \\
        9 & Protection & 36 & 66.7 \\
        10 & Other & 15 & 53.3 \\
        \midrule
        \multicolumn{2}{l}{\textbf{Overall}} & \textbf{3,885} & \textbf{80.9} \\
        \bottomrule
    \end{tabular}
\end{table}


\end{document}
