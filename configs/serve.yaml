server:
  host: "0.0.0.0"
  port: 8000
  workers: 1  # Single worker â€” GPU inference is the bottleneck
  # warmup_models: ["llm", "graph_head", "forward"]  # Uncomment to pre-load at startup

models:
  llm:
    checkpoint: "checkpoints/llm/uspto50k_v6/final"
    base_weights: "weights/rsgpt/finetune_50k.pth"  # RSGPT base model weights
    device: "auto"
  retro_v2:
    checkpoint: "checkpoints/retro_v2/uspto50k/best/model.pt"
    device: "auto"
  graph_head:
    checkpoint: "checkpoints/graph_head/best_model.pt"
    device: "auto"
  forward:
    checkpoint: "checkpoints/forward/uspto50k/best_model.pt"
    device: "auto"

data:
  lg_vocab: "data/vocab/lg_vocab.json"
  inventory: null  # Optional path to building_blocks.txt

inference:
  single_step:
    top_k_bonds: 10
    top_m_lgs: 3
    candidates_per_edit: 5
    top_n_results: 10
    beam_size: 5
  multi_step:
    max_depth: 10
    max_nodes: 5000
    max_time_seconds: 300
    beam_width: 20
